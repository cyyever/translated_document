# The Google File System
#### Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung<br/><div style="text-align:center">google*</div>

*：这些作者可以通过以下地址联系：{sanjay,hgobioff,shuntak}@google.com

# 摘要（ABSTRACT）
我们设计并实现了Google文件系统，一个可扩展的分布式文件系统，用于大型的数据密集型的分布式应用程序。它运行于不昂贵的硬件上，但提供了容错性，并且它可以提供了高的整体性能（aggregate performance）给大量的客户端。虽然GFS有很多目标和以前的分布式文件系统一样，我们根据对（当前和预计的）应用程序工作负载（workload）和技术环境的观察来引导GFS的设计，这些观察反映我们的设计不同于和以前一些对文件系统的假设。这导致我们重新审视传统的设计选项并探索完全不同的设计要点。

由此产生的文件系统成功地满足了我们的存储需求。它在Google内部被广泛部署，我们那些需要大数据集的服务或研究和开发工作把作为存储平台来生成和处理数据。其中最大的数据集群提供了数百TB的存储能力，这些存储能力由超过一千台机器上的数千个硬盘提供，并且该集群被数百个客户端并发访问。

在本论文里我们提供了为了支持分布式应用而设计的文件系统接口的扩展，从许多方面讨论了我们设计，同时报告了来自于微型基准测试和真实场景的测量结果。
 
# 分类和主题描述符（Categories and Subject Descriptors）
D [4]: 3—分布式文件系统

# 概括术语（General Terms）
设计，可靠性，性能，测量（Design, reliability, performance, measurement）

# 关键词（General Terms）
容错性，可扩展性，数据存储，集群存储（Fault tolerance, scalability, data storage, clustered storage）

## 1. 介绍（INTRODUCTION）
我们设计并实现了Google文件系统（GFS）来满足Google急剧增长的数据处理需求。GFS有很多目标和以前的分布式文件系统一样，例如性能，可扩展性，可靠性和可用性。但是它的设计是根据我们对（当前和预计的）应用程序工作负载（workload）和技术环境的观察来引导的，这些观察反映我们的设计不同于和以前一些对文件系统的设计假设。我们重新审视了传统的设计选项并探索了完全不同的设计要点。

首先，组件的故障是常态而非意外。GFS文件系统包含数百以至数千的由不昂贵的零件组成的存储机器，并且被这些数量级的客户端机器访问。组件的质量和数量事实上导致了在任意给定的时间点总有一些机器出现功能故障并且有一些机器无法从当前的故障中恢复。我们碰到过应用程序BUG，操作系统BUG, 人类错误和磁盘，内存，插头和电力供应的故障所导致的问题。因此，持续地监控，错误检测，容错性和自动恢复必须集成到文件系统里。
 
第二，按照传统的标准，我们所涉及的文件是超大文件。几GB的文件是家常便饭。每个文件通常包含许多的程序对象，例如web文档。当我们例行要处理的是快速增长的许多TB的数据集（包含若干亿的程序对象）时，就算是文件系统提供支持，我们也难以有效管理若干亿只有kb大小的文件。
因此，文件系统的设计假定和参数，例如I/O操作和块大小（block size），必须被重新审视。

第三，大多数文件是通过添加（append）新数据而非覆写已有数据来进行修改的。在文件内的随机写几乎是不存在的。一旦写入完毕，文件只被读取，而且经常只被顺序读取。许多类型的数据都有这些特征。有些可能构成大数据仓库以便数据分析程序进行扫描。有些可能是由运行中的程序持续生成的数据流。有些可能是归档数据。有些可能是一台机器产生的中间结果，并同时或者以后被另一台机器处理。由于该超大文件的访问模式，添加（appending）成为了性能优化和原子性保证所关注的焦点，与此同时在客户端缓存数据块的方案不再具有吸引力。

第四，协设计（co-designing）应用程序和文件系统API可以增加我们的灵活性，使得整体系统受益。例如，我们放宽了GFS的一致性模型，这样大大简化了文件系统，但却没有给应用程序增加繁重的负担。我们还引入了一个原子添加的操作以便多个客户端可以并发地添加同一个文件，而不需要在它们之间做额外的同步。我们将在论文后面更详细地讨论这些东西。

目前我们部署了多个GFS集群，用于不同的目的。其中最大的集群拥有超过1000个存储节点，超过300TB的硬盘存储，并且被不同机器上的数百个客户端持续地，繁忙地访问。

## 2. 设计概览（DESIGN OVERVIEW）

### 2.1 假设（Assumptions）
当根据我们的需求设计一个文件系统时，我们被一些假设所指引，这些假设既给予我们挑战，也给予我们机遇。我们在早先聊到了一些关键的观察结论，现在我们将更详细地铺陈我们的假设。

* 系统由许多不昂贵的经常故障的组件构造而成。它必须持续的监控自身，并且探测到例常的组件故障，容忍这些故障，并且从中恢复。
* 系统存储适量的大文件。我们期望存储几百万文件，每个有通常100MB或更大。存储几GB的文件对我们来说是常态，因此必须有效地管理它们。必须支持小文件存储，但我们无须为它们做优化。
* 工作负载主要包含两种读操作：大数据量的流式读取（streaming read）和小数据量的随机读取。在大数据量的流式读取里，单个读操作通常读取几百KB，而且经常读取1MB或更多字节。同一个客户端发起的连续操作经常顺序读取一个文件的一个连续区域。一个小数据量的随机读取通常在某个任意的文件偏移量下读取几KB。关注自身性能的应用程序经常对它们的小数据量读取操作进行排序并批量执行，以便稳步向前读取，而非在文件前面和后面跳来跳去地读取。
* 工作负载还包含许多大数据量的顺序写入，它们把数据添加到文件尾部。通常写入的数据量类似于上述流式读取操作的数据量。一旦写入完毕，文件极少再去修改。系统当然也支持在文件任意位置的小数据量的写入，但是这些操作无须高效。
* 系统必须为多个客户端并发地朝同一个文件添加数据的情况来高效地实现良定义（well-defined）的语义。我们的文件经常用作生产者-消费者队列，或者用于多路归并。几百个生产者（每台机器上跑一个生产者）将并发地往一个文件添加数据。必须用最小的同步开销来实现原子添加操作。该文件可能在以后被读取，或者在此时一个消费者正在顺序地读取它。
* 持久的高带宽比低延迟更重要。我们大部分的目标应用程序更注重高速地处理成块的数据，只有少数程序对单次读取或者写入操作有严格的响应时间要求。

### 2.2 接口（Interface）
尽管GFS没实现一个标准的API（例如POSIX），它还是提供了一套熟悉的文件系统接口。把文件放在目录里面构成分层结构，并且用路径名进行标识。我们支持常见的操作来创建，删除，打开，关闭，读取和写入文件。

此外GFS拥有快照（snapshot）和记录添加（record append）的操作。快照以低开销来创建文件或者目录树的副本。记录添加允许多个客户端并发地往同一个文件添加数据，与此同时保证每个客户端的添加操作的原子性。这对于实现多路归并结果和生产者-消费者队列是很有用的，因为许多客户端可以同时添加文件而无须额外的锁定。我们发现这些类型的文件对构建大型的分布式应用程序有着无法估计的作用。我们将分别在3.4节和3.3节进一步讨论快照和记录添加。
 
### 2.3 架构（Architecture）
一个GFS集群由单台*主服务器*（master）和多台*数据块服务器*（chunkserver）构成（译者注：后面段落出现master和chunkserver不再翻译），并被许多*客户端*访问，如图１所示。这些服务器通常是不昂贵的linux机器，其上运行着用户态的服务器进程。在同一台机器上运行chunkserver和客户端是很容易的事情，只要机器的资源允许，并且可以接受可能存在异常行为的应用程序在运行时会导致机器可靠性降低的情况。

文件被分成固定大小的*数据块*（chunk）。master会在创建每个数据块之时分配一个全局唯一的，并且不可修改的64位的*数据块句柄*（chunk handle）给它，来标识这个数据块。Chunkserver把这些数据库当作Linux文件存储在本地硬盘，并且通过指定的数据库句柄和字节范围来读取和写入数据块的数据。为了达到可靠性，每个数据块被复制在多个chunkserver上面。默认情况下，我们存储三个副本，但是用户可以为文件命名空间（file namespace）的不同区域来指定不同的复制级别。

master维护所有的文件系统元数据。这些元数据包含了命名空间，访问控制信息，从文件到数据块的映射关系以及数据块的当前位置（译者注：也就是数据块的所有副本所在的多个chunkserver）。它还控制了整个文件系统范围内的操作，例如数据块凭据（lease）的管理，孤儿数据块的垃圾回收以及数据块在chunkserver之间搬迁。Master定时地通过*心跳*（HeartBeat）消息来与每个chunkserver通信，以便把指令给予chunkserver或者收集它的状态。

GFS的客户端代码被链接到每个应用程序，它实现了该文件系统的API，并且代表应用程序和master以及chunkserver进行通信，以便读取和写入数据。对于元数据的操作，客户端和master进行交互，但是数据相关的所有通讯是直接在客户端和chunkserver之间进行的。我们并不提供POSIX API，因此不需要给linux的vnode层增加钩子。

客户端和chunkserver都不缓存文件数据。客户端缓存没有什么用，因为大部分的应用程序要么流式地读取超大文件，要么所处理的工作集（working set）太大了，以至于无法缓存。不缓存文件数据简化了客户端和总体系统的实现，因为这样就不存在缓存一致性的问题。（但是客户端确实有缓存元数据。）Chunkserver不需要缓存文件数据的原因是数据块是作为本地文件来存储的，因此Linux的buffer cache（译者注：Linux内核把硬盘的物理块缓存到内存的机制，可以实现预读等功能）已经把频繁访问的数据放在内存中了。

### 2.4 单台主服务器（Single Master）
拥有单台master极大地简化了我们的设计，并且使得master可以使用全局知识来进行数据块放置和复制的复杂决策。但是我们必须最小化master对数据的读取和写入的介入以便它不会成为瓶颈。客户端从不通过master来读取和写入文件数据。客户端只不过询问master它需要联系哪些chunkserver。对于随后的许多操作来说，客户端会把询问结果缓存一段有限的时间，然后直接和chunkserver交互。让我们参考图１来解释一个简单的读取操作所涉及到的交互。首先，通过固定的数据块大小，客户端把应用程序指定的文件名和字节偏移量转换成文件内的数据块索引。然后它给master发送一个包含文件名和数据块索引的请求。master用对应的数据块句柄和数据块所有副本的位置来响应该请求。客户端使用文件名和数据块索引作为key值来缓存这个信息。然后客户端发送一个请求给其中一个副本（最可能是距离最近的副本）。这个请求指定了数据块句柄以及在该数据块内的一个字节范围。此后在同一个数据块内的读取操作不再需要客户端和master的交互，除非客户端缓存的信息到期了，或者那个文件被重新打开。事实上客户端通常会在同一个请求内询问多个数据块，master也会把紧接在请求的数据块之后的数据块的信息也放在响应中。这些额外的信息省去了之后的好几次客户端和master的交互，但却几乎不造成额外的开销。
 
### 2.5 数据块大小（Chunk Size）
数据块大小是一个关键的设计参数。我们选择了64MB，这比常见的文件系统块大小大得多。每个数据块的副本作为一个普通的Linux文件存放在一个chunkserver上，并且只在有需要时进行扩展。惰性空间分配避免了文件内部的碎片造成的空间浪费，内部碎片也许是反对这么大的数据块大小的一个最大的理由。

一个大的数据块大小有好几个重要的优点。首先，它降低了客户端需要和master交互的需要，因为在同一个数据块内读取和写入只需要在一开始向master发送一次关于数据块位置的请求。对于我们的工作负载来说，这个交互需求的降低有着特别重要的意义，因为在大多数情况下我们的应用程序顺序地读取或者写入大文件。就算是对于小数据量的随机读取来说，客户端也能轻松地缓存一个有许多TB的工作集的所有数据块的位置信息。第二，由于使用大数据块，一个客户端更有可能在一个给定的数据块上执行许多操作，它可以通过在一段较长的时间内维持和一个chunkserver的TCP长连接来降低网络开销。第三，它降低了master所存储的元数据的大小。这使得我们可以把元数据放在master的内存中，这进一步带来了其它的优点，我们将在2.6.1节讨论这些优点。

但在另一方面，就算已经有了惰性空间分配，一个大的数据块大小仍然有其缺点。一个小文件包含少量的数据块，可能只有一个数据块。如果许多客户端在访问同一个文件，存放这些数据块的chunkserver可能成为热点。在实际情况中，热点并不是一个大问题，因为我们的应用程序在大多数情况下顺序地读取有多个数据块的大文件。

但是当GFS第一次用于一个batch-queue系统时，热点确实是个问题：一个可执行程序被写入到GFS,它的文件只有一个数据块，然后同时在数百台机器上启动这个程序。存储这个可执行程序的少量chunkserver被数百个同时出现的请求搞得过载。我们通过使用一个更高的复制参数来存储这种可执行程序，以及让batch-queue系统错开程序启动的时刻来解决这个问题。一个可能的长期解决方案是允许客户端在这种情况下从其它客户端读取数据。

### 2.6 元数据（Metadata）
Master存储三种主要的元数据：文件命名空间和数据块命名空间，从文件到数据块的映射，每个数据块的副本的位置。所有的元数据被放在master的内存中。通过把变更写入到一个操作日志（operation log）的方式，前两种类型（命名空间和文件到数据块的映射）的元数据也被持久存储，这个操作日志保持在master的本地硬盘，同时在远程机器上保存副本。使用一个日志允许我们

The master stores three major types of metadata: the file
and chunk namespaces, the mapping from files to chunks,
and the locations of each chunk’s replicas. All metadata is
kept in the master’s memory. The first two types (names-
paces and file-to-chunk mapping) are also kept persistent by
logging mutations to an operation log stored on the mas-
ter’s local disk and replicated on remote machines. 

Using
a log allows us to update the master state simply, reliably,
and without risking inconsistencies in the event of a master
crash. The master does not store chunk location informa-
tion persistently. Instead, it asks each chunkserver about its
chunks at master startup and whenever a chunkserver joins
the cluster.

 
